{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('env1': conda)",
   "display_name": "Python 3.8.5 64-bit ('env1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6014555cf6b39fd438d8e191e5dd063380baf8c72a384cd5d5f2a2f337222208"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from xlwt import Workbook\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wapiti'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8519e74f4955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mnormal_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mparagraphs_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_paragraphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0msentences_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraphs_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[0mrandom_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraphs_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mfine_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-8519e74f4955>\u001b[0m in \u001b[0;36msentences_count\u001b[1;34m(paragraphs_list)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0msentence_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mword_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOSTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"resources/postagger.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\env1\\lib\\site-packages\\hazm\\SequenceTagger.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, patterns, **options)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[1;32mfrom\u001b[0m \u001b[0mwapiti\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wapiti'"
     ]
    }
   ],
   "source": [
    "def normalizing():\n",
    "    # writer armin,reza,sobhan\n",
    "\n",
    "    with open(\"./group8text.txt\", 'r', encoding=\"utf8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "        normalizer = Normalizer(token_based=True)\n",
    "        normal_text = normalizer.punctuation_spacing(normalizer.character_refinement(normalizer.affix_spacing(text)))\n",
    "\n",
    "        return normal_text\n",
    "\n",
    "\n",
    "def sentences_count(paragraphs_list):\n",
    "    # writer reza\n",
    "\n",
    "    wb = Workbook()\n",
    "    sheet1 = wb.add_sheet('Sheet 1')\n",
    "    sheet1.write(0, 0, 'شماره پاراگراف')\n",
    "    sheet1.write(0, 1, 'تعداد جملات')\n",
    "    sheet1.write(0, 2, 'تعداد کلمات')\n",
    "    sheet1.write(0, 3, 'تعداد فعل ها')\n",
    "    sheet1.write(0, 4, 'تعداد اسم ها')\n",
    "\n",
    "    sentence_tokenizer = SentenceTokenizer()\n",
    "    word_tokenizer = WordTokenizer()\n",
    "    tagger = POSTagger(model=\"resources/postagger.model\")\n",
    "\n",
    "    \n",
    "    for i in range(len(paragraphs_list)):\n",
    "        sheet1.write(i + 1, 0, i + 1)\n",
    "        sheet1.write(i + 1, 1, len(sentence_tokenizer.tokenize(paragraphs_list[i])))\n",
    "        token_list = word_tokenizer.tokenize(paragraphs_list[i])\n",
    "        sheet1.write(i + 1, 2, len(token_list))\n",
    "\n",
    "        tup_list = tagger.tag(token_list)\n",
    "        verb_list = [item for item in tup_list if item[0] == 'V']\n",
    "        noun_list = [item for item in tup_list if item[0] == 'N']\n",
    "\n",
    "        sheet1.write(i + 1, 3, len(verb_list))\n",
    "    #     sheet1.write(i + 1, 3, len( find verbs  ))\n",
    "        sheet1.write(i + 1, 4, len(noun_list))\n",
    "\n",
    "    wb.save('HW2.xls')\n",
    "\n",
    "\n",
    "def fine_tokens(sentences):\n",
    "    # writer sobhan\n",
    "\n",
    "    word_tokenizer = WordTokenizer()\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenizer.tokenize(sentence)\n",
    "\n",
    "        print(tokens)\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "\n",
    "def random_selection(paragraphs_list):      # ---- select 5 random sentences for other queries ----\n",
    "    # writer sobhan\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    for par in paragraphs_list:  # ---- In order to merge all sentences in one string ----\n",
    "        text += par\n",
    "\n",
    "    random_sentences = []\n",
    "\n",
    "    sentence_tokenizer = SentenceTokenizer()\n",
    "    all_sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "    random_list = random.sample(range(0, len(all_sentences)), 5)  # ---- generate 5 random number\n",
    "\n",
    "    for index in random_list:\n",
    "        random_sentences.append(all_sentences[index])\n",
    "\n",
    "    return random_sentences\n",
    "\n",
    "\n",
    "def split_paragraphs(text):\n",
    "    # writer sobhan\n",
    "\n",
    "    paragraphs = text.split(\"پاراگراف\")[1:]\n",
    "\n",
    "    for i in range(len(paragraphs)):\n",
    "        paragraphs[i] = paragraphs[i][2:]\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    normal_text = normalizing()\n",
    "    paragraphs_list = split_paragraphs(normal_text)\n",
    "    sentences_count(paragraphs_list)\n",
    "    random_sentences = random_selection(paragraphs_list)\n",
    "    fine_tokens(random_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}